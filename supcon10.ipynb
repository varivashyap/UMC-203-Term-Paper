{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKRE2rH0PVrP"
      },
      "source": [
        "# Supervised Contrastive Learning\n",
        "\n",
        "**Author:** [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)<br>\n",
        "**Date created:** 2020/11/30<br>\n",
        "**Last modified:** 2020/11/30<br>\n",
        "**Description:** Using supervised contrastive learning for image classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGqx-6RCPVrQ"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "[Supervised Contrastive Learning](https://arxiv.org/abs/2004.11362)\n",
        "(Prannay Khosla et al.) is a training methodology that outperforms\n",
        "supervised training with crossentropy on classification tasks.\n",
        "\n",
        "Essentially, training an image classification model with Supervised Contrastive\n",
        "Learning is performed in two phases:\n",
        "\n",
        "1. Training an encoder to learn to produce vector representations of input images such\n",
        "that representations of images in the same class will be more similar compared to\n",
        "representations of images in different classes.\n",
        "2. Training a classifier on top of the frozen encoder.\n",
        "\n",
        "Note that this example requires [TensorFlow Addons](https://www.tensorflow.org/addons), which you can install using the following command:\n",
        "\n",
        "```python\n",
        "pip install tensorflow-addons\n",
        "```\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "id": "4fUnk436bzy0",
        "outputId": "f138cf4a-3d66-4341-d919-1fdac0f87969",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.0)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8Z_4-CV6PVrR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etC7WZhvPVrS"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZhH_Bf7JPVrS"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "# # Load the train and test data splits\n",
        "# (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "# # Display shapes of train and test datasets\n",
        "# print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "# print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "# from torchvision import datasets\n",
        "\n",
        "# # Load CIFAR-10 training dataset\n",
        "# cifar10_train = datasets.CIFAR10(root='./data', train=True, download=True)\n",
        "\n",
        "# # Load CIFAR-10 testing dataset\n",
        "# cifar10_test = datasets.CIFAR10(root='./data', train=False, download=True)\n",
        "\n",
        "# # Concatenate the training and testing datasets\n",
        "# cifar10_combined = cifar10_train + cifar10_test\n",
        "\n",
        "\n",
        "# # Convert CIFAR-10 dataset into a list of tuples containing pixel values\n",
        "# cifar10_combined_list = [(image_tensor, label) for image_tensor, label in cifar10_combined]\n",
        "\n",
        "# # Shuffle the combined dataset\n",
        "# random.shuffle(cifar10_combined_list)\n",
        "\n",
        "# # Calculate the number of samples for the first part (k%)\n",
        "# k_percentage = 10  # You can adjust this value as needed\n",
        "# k_samples = int(len(cifar10_combined_list) * k_percentage / 100)\n",
        "\n",
        "# # Split the combined dataset into two parts\n",
        "# cifar10_first_part = cifar10_combined_list[:k_samples]\n",
        "# cifar10_second_part = cifar10_combined_list[k_samples:]\n",
        "\n",
        "# # Optionally, convert the lists back into datasets if needed\n",
        "# cifar10_first_part_dataset = CustomCIFAR10Dataset(cifar10_first_part)\n",
        "# cifar10_second_part_dataset = CustomCIFAR10Dataset(cifar10_second_part)"
      ],
      "metadata": {
        "id": "K4G5C4EZfNkw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from torchvision.transforms.functional import to_tensor\n",
        "\n",
        "# # Initialize lists to store images and labels\n",
        "# x_train = []\n",
        "# y_train = []\n",
        "\n",
        "# # Iterate through cifar10_first_part to extract image tensors and labels\n",
        "# for image, label in cifar10_first_part:\n",
        "#     # Convert image to tensor\n",
        "#     image_tensor = to_tensor(image)\n",
        "#     # Append image tensor to x_train\n",
        "#     x_train.append(image_tensor)\n",
        "#     # Append label to y_train\n",
        "#     y_train.append(label)\n",
        "\n",
        "# # Convert lists to tensors\n",
        "# x_train = torch.stack(x_train)  # Convert list of tensors to a tensor\n",
        "# y_train = torch.tensor(y_train)  # Convert list of labels to a tensor\n",
        "\n",
        "# import torch\n",
        "# from torchvision.transforms.functional import to_tensor\n",
        "\n",
        "# # Initialize lists to store images and labels\n",
        "# x_test = []\n",
        "# y_test = []\n",
        "\n",
        "# # Iterate through cifar10_second_part to extract image tensors and labels\n",
        "# for image, label in cifar10_second_part:\n",
        "#     # Convert image to tensor\n",
        "#     image_tensor = to_tensor(image)\n",
        "#     # Append image tensor to x_test\n",
        "#     x_test.append(image_tensor)\n",
        "#     # Append label to y_test\n",
        "#     y_test.append(label)\n",
        "\n",
        "# # Convert lists to tensors\n",
        "# x_test = torch.stack(x_test)  # Convert list of tensors to a tensor\n",
        "# y_test = torch.tensor(y_test)  # Convert list of labels to a tensor\n",
        "\n"
      ],
      "metadata": {
        "id": "2q5zNzdUf0j1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from torchvision import datasets\n",
        "\n",
        "# Load CIFAR-10 training dataset\n",
        "cifar10_train = datasets.CIFAR10(root='./data', train=True, download=True)\n",
        "\n",
        "# Load CIFAR-10 testing dataset\n",
        "cifar10_test = datasets.CIFAR10(root='./data', train=False, download=True)\n",
        "\n",
        "# Concatenate the training and testing datasets\n",
        "cifar10_combined = cifar10_train + cifar10_test\n",
        "\n",
        "\n",
        "# Define the percentage split\n",
        "k = 10  # Change this to the desired percentage split\n",
        "\n",
        "# Split the combined CIFAR-10 dataset\n",
        "x_data = [np.array(data[0]) for data in cifar10_combined]\n",
        "y_data = [data[1] for data in cifar10_combined]\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, train_size=k/100, random_state=42)\n",
        "\n",
        "# Convert to numpy arrays\n",
        "x_train = np.array(x_train)\n",
        "x_test = np.array(x_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Print shapes to verify\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "41qN6B_WkmQP",
        "outputId": "c7b37ed4-07b1-405f-d258-aab06dd60d82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "x_train shape: (6000, 32, 32, 3)\n",
            "y_train shape: (6000,)\n",
            "x_test shape: (54000, 32, 32, 3)\n",
            "y_test shape: (54000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjaLCKUiPVrS"
      },
      "source": [
        "## Using image data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DWBTwfc7PVrT"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.02),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Setting the state of the normalization layer.\n",
        "data_augmentation.layers[0].adapt(x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lwci2WkxPVrT"
      },
      "source": [
        "## Build the encoder model\n",
        "\n",
        "The encoder model takes the image as input and turns it into a 2048-dimensional\n",
        "feature vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "hPnVhtJFPVrT",
        "outputId": "cd97f6b9-5b04-4a3c-9447-63c53f9fec0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"cifar10-encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_7 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " sequential_1 (Sequential)   (None, 32, 32, 3)         7         \n",
            "                                                                 \n",
            " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23564807 (89.89 MB)\n",
            "Trainable params: 23519360 (89.72 MB)\n",
            "Non-trainable params: 45447 (177.53 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def create_encoder():\n",
        "    resnet = keras.applications.ResNet50V2(\n",
        "        include_top=False, weights=None, input_shape=input_shape, pooling=\"avg\"\n",
        "    )\n",
        "\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    augmented = data_augmentation(inputs)\n",
        "    outputs = resnet(augmented)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-encoder\")\n",
        "    return model\n",
        "\n",
        "\n",
        "encoder = create_encoder()\n",
        "encoder.summary()\n",
        "\n",
        "learning_rate = 0.001\n",
        "batch_size = 265\n",
        "hidden_units = 512\n",
        "projection_units = 128\n",
        "num_epochs = 20\n",
        "dropout_rate = 0.5\n",
        "temperature = 0.05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmLmL5dYPVrT"
      },
      "source": [
        "## Build the classification model\n",
        "\n",
        "The classification model adds a fully-connected layer on top of the encoder,\n",
        "plus a softmax layer with the target classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "slSh4yqvPVrT"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_classifier(encoder, trainable=True):\n",
        "\n",
        "    for layer in encoder.layers:\n",
        "        layer.trainable = trainable\n",
        "\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    features = encoder(inputs)\n",
        "    features = layers.Dropout(dropout_rate)(features)\n",
        "    features = layers.Dense(hidden_units, activation=\"relu\")(features)\n",
        "    features = layers.Dropout(dropout_rate)(features)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(features)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-classifier\")\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYrfmZ1-PVrU"
      },
      "source": [
        "## Experiment 1: Train the baseline classification model\n",
        "\n",
        "In this experiment, a baseline classifier is trained as usual, i.e., the\n",
        "encoder and the classifier parts are trained together as a single model\n",
        "to minimize the crossentropy loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "F_5eEFZbPVrU",
        "outputId": "face9767-3bb9-498c-ef5d-05d585f4f9c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"cifar10-classifier\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_10 (InputLayer)       [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " cifar10-encoder (Functiona  (None, 2048)              23564807  \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               1049088   \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24619025 (93.91 MB)\n",
            "Trainable params: 24573578 (93.74 MB)\n",
            "Non-trainable params: 45447 (177.53 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "23/23 [==============================] - 52s 329ms/step - loss: 2.5555 - sparse_categorical_accuracy: 0.1493\n",
            "Epoch 2/20\n",
            "23/23 [==============================] - 2s 94ms/step - loss: 2.0995 - sparse_categorical_accuracy: 0.2205\n",
            "Epoch 3/20\n",
            "23/23 [==============================] - 2s 94ms/step - loss: 1.8936 - sparse_categorical_accuracy: 0.2955\n",
            "Epoch 4/20\n",
            "23/23 [==============================] - 2s 94ms/step - loss: 1.7354 - sparse_categorical_accuracy: 0.3548\n",
            "Epoch 5/20\n",
            "23/23 [==============================] - 2s 97ms/step - loss: 1.5747 - sparse_categorical_accuracy: 0.4198\n",
            "Epoch 6/20\n",
            "23/23 [==============================] - 2s 96ms/step - loss: 1.4773 - sparse_categorical_accuracy: 0.4573\n",
            "Epoch 7/20\n",
            "23/23 [==============================] - 2s 95ms/step - loss: 1.3469 - sparse_categorical_accuracy: 0.5148\n",
            "Epoch 8/20\n",
            "23/23 [==============================] - 2s 95ms/step - loss: 1.2776 - sparse_categorical_accuracy: 0.5513\n",
            "Epoch 9/20\n",
            "23/23 [==============================] - 2s 96ms/step - loss: 1.1850 - sparse_categorical_accuracy: 0.5837\n",
            "Epoch 10/20\n",
            "23/23 [==============================] - 2s 97ms/step - loss: 1.1533 - sparse_categorical_accuracy: 0.5923\n",
            "Epoch 11/20\n",
            "23/23 [==============================] - 2s 99ms/step - loss: 1.0801 - sparse_categorical_accuracy: 0.6252\n",
            "Epoch 12/20\n",
            "23/23 [==============================] - 2s 96ms/step - loss: 1.0039 - sparse_categorical_accuracy: 0.6518\n",
            "Epoch 13/20\n",
            "23/23 [==============================] - 2s 97ms/step - loss: 0.9409 - sparse_categorical_accuracy: 0.6785\n",
            "Epoch 14/20\n",
            "23/23 [==============================] - 2s 98ms/step - loss: 0.8900 - sparse_categorical_accuracy: 0.6890\n",
            "Epoch 15/20\n",
            "23/23 [==============================] - 2s 96ms/step - loss: 0.8727 - sparse_categorical_accuracy: 0.7032\n",
            "Epoch 16/20\n",
            "23/23 [==============================] - 2s 100ms/step - loss: 0.8273 - sparse_categorical_accuracy: 0.7190\n",
            "Epoch 17/20\n",
            "23/23 [==============================] - 2s 98ms/step - loss: 0.7696 - sparse_categorical_accuracy: 0.7382\n",
            "Epoch 18/20\n",
            "23/23 [==============================] - 2s 98ms/step - loss: 0.7144 - sparse_categorical_accuracy: 0.7588\n",
            "Epoch 19/20\n",
            "23/23 [==============================] - 2s 99ms/step - loss: 0.7745 - sparse_categorical_accuracy: 0.7450\n",
            "Epoch 20/20\n",
            "23/23 [==============================] - 2s 99ms/step - loss: 0.8424 - sparse_categorical_accuracy: 0.7163\n",
            "1688/1688 [==============================] - 21s 11ms/step - loss: 2.3622 - sparse_categorical_accuracy: 0.4078\n",
            "Test accuracy: 40.78%\n"
          ]
        }
      ],
      "source": [
        "encoder = create_encoder()\n",
        "classifier = create_classifier(encoder)\n",
        "classifier.summary()\n",
        "\n",
        "history = classifier.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs)\n",
        "\n",
        "accuracy = classifier.evaluate(x_test, y_test)[1]\n",
        "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yrvISEvPVrU"
      },
      "source": [
        "## Experiment 2: Use supervised contrastive learning\n",
        "\n",
        "In this experiment, the model is trained in two phases. In the first phase,\n",
        "the encoder is pretrained to optimize the supervised contrastive loss,\n",
        "described in [Prannay Khosla et al.](https://arxiv.org/abs/2004.11362).\n",
        "\n",
        "In the second phase, the classifier is trained using the trained encoder with\n",
        "its weights freezed; only the weights of fully-connected layers with the\n",
        "softmax are optimized.\n",
        "\n",
        "### 1. Supervised contrastive learning loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7i2OZp0-PVrU"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
        "    def __init__(self, temperature=1, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
        "        # Normalize feature vectors\n",
        "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
        "        # Compute logits\n",
        "        logits = tf.divide(\n",
        "            tf.matmul(\n",
        "                feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
        "            ),\n",
        "            self.temperature,\n",
        "        )\n",
        "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)\n",
        "\n",
        "\n",
        "def add_projection_head(encoder):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    features = encoder(inputs)\n",
        "    outputs = layers.Dense(projection_units, activation=\"relu\")(features)\n",
        "    model = keras.Model(\n",
        "        inputs=inputs, outputs=outputs, name=\"cifar-encoder_with_projection-head\"\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nlqY_uWPVrU"
      },
      "source": [
        "### 2. Pretrain the encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "7fAaXRi9PVrU",
        "outputId": "33acda2d-010f-46e6-944a-d8566455e674",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"cifar-encoder_with_projection-head\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_13 (InputLayer)       [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " cifar10-encoder (Functiona  (None, 2048)              23564807  \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               262272    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23827079 (90.89 MB)\n",
            "Trainable params: 23781632 (90.72 MB)\n",
            "Non-trainable params: 45447 (177.53 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "23/23 [==============================] - 27s 100ms/step - loss: 5.7736\n",
            "Epoch 2/20\n",
            "23/23 [==============================] - 2s 98ms/step - loss: 5.3742\n",
            "Epoch 3/20\n",
            "23/23 [==============================] - 2s 99ms/step - loss: 5.2636\n",
            "Epoch 4/20\n",
            "23/23 [==============================] - 2s 98ms/step - loss: 5.1974\n",
            "Epoch 5/20\n",
            "23/23 [==============================] - 2s 98ms/step - loss: 5.1327\n",
            "Epoch 6/20\n",
            "23/23 [==============================] - 2s 98ms/step - loss: 5.0813\n",
            "Epoch 7/20\n",
            "23/23 [==============================] - 2s 99ms/step - loss: 5.0166\n",
            "Epoch 8/20\n",
            "23/23 [==============================] - 2s 101ms/step - loss: 4.9868\n",
            "Epoch 9/20\n",
            "23/23 [==============================] - 2s 100ms/step - loss: 4.9381\n",
            "Epoch 10/20\n",
            "23/23 [==============================] - 2s 100ms/step - loss: 4.8912\n",
            "Epoch 11/20\n",
            "23/23 [==============================] - 2s 100ms/step - loss: 4.7969\n",
            "Epoch 12/20\n",
            "23/23 [==============================] - 2s 100ms/step - loss: 4.7751\n",
            "Epoch 13/20\n",
            "23/23 [==============================] - 2s 104ms/step - loss: 4.7394\n",
            "Epoch 14/20\n",
            "23/23 [==============================] - 2s 101ms/step - loss: 4.6979\n",
            "Epoch 15/20\n",
            "23/23 [==============================] - 2s 100ms/step - loss: 4.6343\n",
            "Epoch 16/20\n",
            "23/23 [==============================] - 2s 100ms/step - loss: 4.5819\n",
            "Epoch 17/20\n",
            "23/23 [==============================] - 2s 100ms/step - loss: 4.5453\n",
            "Epoch 18/20\n",
            "23/23 [==============================] - 2s 102ms/step - loss: 4.5002\n",
            "Epoch 19/20\n",
            "23/23 [==============================] - 2s 100ms/step - loss: 4.4571\n",
            "Epoch 20/20\n",
            "23/23 [==============================] - 2s 99ms/step - loss: 4.3981\n"
          ]
        }
      ],
      "source": [
        "encoder = create_encoder()\n",
        "\n",
        "encoder_with_projection_head = add_projection_head(encoder)\n",
        "encoder_with_projection_head.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate),\n",
        "    loss=SupervisedContrastiveLoss(temperature),\n",
        ")\n",
        "\n",
        "encoder_with_projection_head.summary()\n",
        "\n",
        "history = encoder_with_projection_head.fit(\n",
        "    x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCQgc-U6PVrW"
      },
      "source": [
        "### 3. Train the classifier with the frozen encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "h4cVNFtMPVrW",
        "outputId": "1bd403b6-27f8-46a5-b283-7f55f555e287",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "23/23 [==============================] - 4s 31ms/step - loss: 2.0888 - sparse_categorical_accuracy: 0.5878\n",
            "Epoch 2/20\n",
            "23/23 [==============================] - 1s 28ms/step - loss: 1.3859 - sparse_categorical_accuracy: 0.6387\n",
            "Epoch 3/20\n",
            "23/23 [==============================] - 1s 28ms/step - loss: 1.1480 - sparse_categorical_accuracy: 0.6520\n",
            "Epoch 4/20\n",
            "23/23 [==============================] - 1s 26ms/step - loss: 1.0623 - sparse_categorical_accuracy: 0.6625\n",
            "Epoch 5/20\n",
            "23/23 [==============================] - 1s 25ms/step - loss: 0.9864 - sparse_categorical_accuracy: 0.6607\n",
            "Epoch 6/20\n",
            "23/23 [==============================] - 1s 26ms/step - loss: 0.9457 - sparse_categorical_accuracy: 0.6707\n",
            "Epoch 7/20\n",
            "23/23 [==============================] - 1s 25ms/step - loss: 0.9279 - sparse_categorical_accuracy: 0.6752\n",
            "Epoch 8/20\n",
            "23/23 [==============================] - 1s 25ms/step - loss: 0.9657 - sparse_categorical_accuracy: 0.6683\n",
            "Epoch 9/20\n",
            "23/23 [==============================] - 1s 25ms/step - loss: 0.9085 - sparse_categorical_accuracy: 0.6770\n",
            "Epoch 10/20\n",
            "23/23 [==============================] - 1s 26ms/step - loss: 0.8940 - sparse_categorical_accuracy: 0.6865\n",
            "Epoch 11/20\n",
            "23/23 [==============================] - 1s 25ms/step - loss: 0.9118 - sparse_categorical_accuracy: 0.6768\n",
            "Epoch 12/20\n",
            "23/23 [==============================] - 1s 26ms/step - loss: 0.9175 - sparse_categorical_accuracy: 0.6762\n",
            "Epoch 13/20\n",
            "23/23 [==============================] - 1s 25ms/step - loss: 0.8648 - sparse_categorical_accuracy: 0.6915\n",
            "Epoch 14/20\n",
            "23/23 [==============================] - 1s 25ms/step - loss: 0.8764 - sparse_categorical_accuracy: 0.6872\n",
            "Epoch 15/20\n",
            "23/23 [==============================] - 1s 25ms/step - loss: 0.8759 - sparse_categorical_accuracy: 0.6855\n",
            "Epoch 16/20\n",
            "23/23 [==============================] - 1s 26ms/step - loss: 0.8753 - sparse_categorical_accuracy: 0.6903\n",
            "Epoch 17/20\n",
            "23/23 [==============================] - 1s 26ms/step - loss: 0.8646 - sparse_categorical_accuracy: 0.6863\n",
            "Epoch 18/20\n",
            "23/23 [==============================] - 1s 25ms/step - loss: 0.8503 - sparse_categorical_accuracy: 0.6963\n",
            "Epoch 19/20\n",
            "23/23 [==============================] - 1s 25ms/step - loss: 0.8523 - sparse_categorical_accuracy: 0.6895\n",
            "Epoch 20/20\n",
            "23/23 [==============================] - 1s 25ms/step - loss: 0.8656 - sparse_categorical_accuracy: 0.6923\n",
            "1688/1688 [==============================] - 19s 10ms/step - loss: 1.6579 - sparse_categorical_accuracy: 0.4806\n",
            "Test accuracy: 48.06%\n"
          ]
        }
      ],
      "source": [
        "classifier = create_classifier(encoder, trainable=False)\n",
        "\n",
        "history = classifier.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=num_epochs)\n",
        "\n",
        "accuracy = classifier.evaluate(x_test, y_test)[1]\n",
        "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onQ4azHePVrW"
      },
      "source": [
        "We get to an improved test accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_4oIoJ7PVrW"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "As shown in the experiments, using the supervised contrastive learning technique\n",
        "outperformed the conventional technique in terms of the test accuracy. Note that\n",
        "the same training budget (i.e., number of epochs) was given to each technique.\n",
        "Supervised contrastive learning pays off when the encoder involves a complex\n",
        "architecture, like ResNet, and multi-class problems with many labels.\n",
        "In addition, large batch sizes and multi-layer projection heads\n",
        "improve its effectiveness. See the [Supervised Contrastive Learning](https://arxiv.org/abs/2004.11362)\n",
        "paper for more details.\n",
        "\n",
        "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/supervised-contrastive-learning-cifar10)\n",
        "and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/supervised-contrastive-learning)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}